// ========================================================================================
//  GRSFramework 
//  Copyright (C) 2016 by Gabriel Nützi <gnuetzi (at) gmail (døt) com> 
// 
//  This Source Code Form is subject to the terms of the GNU General Public License as 
//  published by the Free Software Foundation; either version 3 of the License,
//  or (at your option) any later version. If a copy of the GPL was not distributed with
//  this file, you can obtain one at http://www.gnu.org/licenses/gpl-3.0.html.
// ========================================================================================

#include "GRSF/dynamics/inclusion/InclusionCommunicator.hpp"

#include "GRSF/common/CommonFunctions.hpp"

#include "GRSF/dynamics/inclusion/ContactGraphMPI.hpp"
#include "GRSF/dynamics/inclusion/ContactGraphVisitorsMPI.hpp"

#include "GRSF/dynamics/general/MPICommunicatorId.hpp"

#include <functional>
#include <numeric>

template<typename TCombo>
InclusionCommunicator<TCombo>::InclusionCommunicator(std::shared_ptr< BodyCommunicator> pBodyComm,
                                                     std::shared_ptr< DynamicsSystemType> pDynSys ,
                                                     std::shared_ptr< ProcessCommunicatorType > pProcComm):

    m_messageContact(this),m_messageMultiplicity(this),
    m_messageSplitBodyUpdate(this), m_messageSplitBodySolution(this),
    m_pDynSys(pDynSys),
    m_pProcComm(pProcComm),
    m_pBodyComm(pBodyComm),
    m_rank(m_pProcComm->getRank()),
    m_globalLocal(pDynSys->m_simBodies),
    m_globalRemote(pDynSys->m_remoteSimBodies),
    m_nbDataMap(m_pProcComm->getRank())
    {

    if(Logging::LogManager::getSingleton().existsLog("SimulationLog")) {
        m_pSimulationLog = Logging::LogManager::getSingleton().getLog("SimulationLog");
    } else {
        ERRORMSG("SimulationLog does not yet exist? Did you create it?")
    }
}

template<typename TCombo>
void InclusionCommunicator<TCombo>::reset(){
    resetTopology();
}

template<typename TCombo>
void InclusionCommunicator<TCombo>::resetTopology(){


    m_pProcTopo =  m_pProcComm->getProcTopo();
    m_nbRanks   =  m_pProcComm->getProcTopo()->getNeighbourRanks();

    // Clear Neighbour Map
    m_nbDataMap.clear();

    // Initialize all NeighbourDatas
    for(auto rankIt = m_nbRanks.begin() ; rankIt != m_nbRanks.end(); rankIt++) {
        LOGIC(m_pSimulationLog,"---> InclusionCommunicator: Add neighbour data for process rank: "<<*rankIt<<std::endl;);
        auto res = m_nbDataMap.insert(*rankIt);
        ASSERTMSG(res.second,"Could not insert in m_nbDataMap for rank: " << *rankIt);
    }
    m_pSimulationLog->logMessage("---> InclusionCommunicator: Initialized all NeighbourDatas");


    m_pSimulationLog->logMessage("---> Initialized InclusionCommunicator");
}


template<typename TCombo>
void InclusionCommunicator<TCombo>::communicateRemoteContacts(PREC currentSimulationTime) {


    m_currentSimulationTime = currentSimulationTime;

    m_nbRanksSendRecvLocal.clear();
    m_nbRanksSendRecvRemote.clear();


    m_messageContact.setTime(m_currentSimulationTime);
    m_messageMultiplicity.setTime(m_currentSimulationTime);
    m_messageSplitBodyUpdate.setTime(m_currentSimulationTime);
    m_messageSplitBodySolution.setTime(m_currentSimulationTime);


    /// First for each neighbour, communicate the ids of all remote body which have contacts
    /// THIS PART CAN ALSO BE DONE WITH A All to Root and Root to All Communication? ========================
    /// Maybe try this, Root to All might get inefficient because , each rank needs a different message
    LOGIC(m_pSimulationLog,"\t---> InclusionCommunication: Communicate Remote Contacts at t="<<m_currentSimulationTime<< std::endl;)
    //TODO
    sendContactMessageToNeighbours(); //TODO Send only message to neighbour if there are remotes in bodyCommunicators neighbour map
    receiveContactMessagesFromNeighbours(); //TODO Receive only message if local bodies are in bodyCommunicators neighbour map


    //Init all weights on the nodes
    LOGIC(m_pSimulationLog,"\t---> Compute Multiplicities"<< std::endl;)
    {
        SorProxInitSplitNodeVisitor<ContactGraphType> v;
        m_pContactGraph->visitSplitNodes(v);
    }

    sendBodyMultiplicityMessageToNeighbours();

    // Recv all multiplicity factor of all remote bodies
    // Weighting factors are applied, initial velocity is calculated for each remote
    recvBodyMultiplicityMessageFromNeighbours();

    /// =======================================================================================================

    // Init all local bodies weights in the split body nodes
    // Weighting factors are applied, initial velocities is NOT calculated, later!
    LOGIC(m_pSimulationLog,"\t---> Apply weights for local bodies in SplitBodyNodes"<< std::endl;)
    {
        SetWeightingLocalBodiesSplitNodeVisitor<ContactGraphType> v;
        m_pContactGraph->visitSplitNodes(v);
    }



    /// Communicate Rank connection for building Communicators (Groups) for the Prox iteration
        buildCommunicatorGroups();
    /// ===========================


    LOGIC(m_pSimulationLog,"---> InclusionCommunication: Communicate Remote finished"<< std::endl;)
}

template<typename TCombo>
void InclusionCommunicator<TCombo>::communicateSplitBodyUpdate(unsigned int globalIterationNumber) {

    m_globalIterationNumber = globalIterationNumber;
    // First for each neighbour, communicate the ids of all remote body which have contacts
    LOGIC(m_pSimulationLog,"---> InclusionCommunication: Communicate Remote Velocities for SplitBodyNode Update ..."<< std::endl;)

    LOGIC(m_pSimulationLog,"\t---> Send Velocities of Remote SplitBodies..."<< std::endl;)
    sendUpdateSplitBodiesToNeighbours();

    LOGIC(m_pSimulationLog,"\t---> Recv Velocities for Local SplitBodies..."<< std::endl;)
    recvUpdateSplitBodiesFromNeighbours();

    LOGIC(m_pSimulationLog,"---> InclusionCommunication: Communicate Remote Velocities finished"<< std::endl;)
}

template<typename TCombo>
void InclusionCommunicator<TCombo>::communicateSplitBodySolution(unsigned int globalIterationNumber) {
    m_globalIterationNumber = globalIterationNumber;

    // First for each neighbour, communicate the ids of all remote body which have contacts
    LOGIC(m_pSimulationLog,"---> InclusionCommunication: Communicate Solved SplitBody Velocity ..."<< std::endl;)

    LOGIC(m_pSimulationLog,"\t---> Send Velocity Solution of Local SplitBodies..."<< std::endl;)
    sendSolutionSplitBodiesToNeighbours();

    LOGIC(m_pSimulationLog,"\t---> Recv Velocity Solution for Remote SplitBodies..."<< std::endl;)
    recvSolutionSplitBodiesFromNeighbours();

    LOGIC(m_pSimulationLog,"---> InclusionCommunication: Communicate Solved SplitBody Velocity finished"<< std::endl;)

}


template<typename TCombo>
void InclusionCommunicator<TCombo>::buildCommunicatorGroups() {
        // Build the communicator groups

        // Group -1 (MPI_UNDEFINED):
        //              - Empty processes (no sim bodies-> no remote contacts possible, only by remote to remote
        //              - processes with no nodes
        //              - processes with only local nodes
        //              They do not need communication during the solving process,
        //              they can just iterate till convergence and thats it!

        // Group 0:     (this group could be split further into sub groups)
        //              - all other processes, with additional remote or splitbody nodes
        //              - They need communication during the solving process
        int groupId;
        if(  m_pContactGraph->isUncoupled() ){
            groupId = -1;
        }else{
            groupId = 0;
        }
        LOGIC(m_pSimulationLog,"MPI> Split Communicator for Convergence Gathering"<<std::endl;)
        m_pProcComm->generateCommunicatorSplitAndOverwrite(MPILayer::MPICommunicatorId::INCLUSION_CONVERGENCE_COMM, groupId);

}

template<typename TCombo>
bool InclusionCommunicator<TCombo>::communicateConvergence(bool converged) {

    LOGIC(m_pSimulationLog,"MPI> Gather convergence flags: "<<std::endl;)
    std::vector<char> convergedGather;
    m_pProcComm->allGather((char)converged,convergedGather, MPILayer::MPICommunicatorId::INCLUSION_CONVERGENCE_COMM);

    //Utilities::printVectorNoCopy(*m_pSimulationLog,convergedGather.begin(),convergedGather.end(),",");

    for(auto it = convergedGather.begin(); it != convergedGather.end(); it++){
        LOGIC(m_pSimulationLog, bool(*it))
    }
    LOGIC(m_pSimulationLog, std::endl;)


     // Sum up all boolean values and decide if all neighbours rebuild?
    converged = std::accumulate(convergedGather.begin(), convergedGather.end() , true, std::logical_and<char>() );

    if(converged){
         LOGIC(m_pSimulationLog,"MPI> converged!"<<std::endl;)
    }else{
         LOGIC(m_pSimulationLog,"MPI> not converged"<<std::endl;)
    }
    return converged;
}


template<typename TCombo>
void InclusionCommunicator<TCombo>::resetAllWeightings() {


    //Reset all local bodies
    ResetWeightingLocalBodiesSplitNodeVisitor<ContactGraphType> v;
    m_pContactGraph->visitSplitNodes(v);

    //Reset weigthing of all remote bodies
    for(auto nbIt = m_nbDataMap.begin(); nbIt != m_nbDataMap.end(); nbIt++){
        for( auto remoteIt = nbIt->second.remoteBegin(); remoteIt != nbIt->second.remoteEnd(); remoteIt++){
            RigidBodyFunctions::changeBodyToNormalWeighting(remoteIt->second.m_pBody);
        }
    }

}



template<typename TCombo>
void InclusionCommunicator<TCombo>::sendContactMessageToNeighbours() {
    LOGIC(m_pSimulationLog,"MPI>\t Send message (EXTERNALCONTACTS_MESSAGE) to neighbours!"<<std::endl;)



    for(auto it = m_nbRanks.begin(); it != m_nbRanks.end(); ++it) {
        LOGIC(m_pSimulationLog,"--->\t\t Send contact message to neighbours with rank: "<< *it <<std::endl;)
        // Instanciate a MessageWrapper which contains a boost::serialization function!
        m_messageContact.setRank(*it);
        m_pProcComm->sendMessageToNeighbourRank_async(m_messageContact,*it, m_messageContact.m_tag);
    }
    LOGIC(m_pSimulationLog,"MPI>\t Send finished!"<<std::endl;)
}
template<typename TCombo>
void InclusionCommunicator<TCombo>::receiveContactMessagesFromNeighbours() {
    if(m_nbRanks.size() == 0){return;}

    LOGIC(m_pSimulationLog,"MPI>\t Receive all messages (EXTERNALCONTACTS_MESSAGE) from neighbours!"<<std::endl;)
    // set the rank of the receiving message automatically! inside the function!
    m_pProcComm->receiveMessageFromRanks(m_messageContact, m_nbRanks, m_messageContact.m_tag );
    LOGIC(m_pSimulationLog,"MPI>\t Receive finished!"<<std::endl;)

    // Wait for all sends to complete, Important because we issue a nonblocking send in sendMessagesToNeighbours
    m_pProcComm->waitForAllNeighbourSends();
}
template<typename TCombo>
void InclusionCommunicator<TCombo>::sendBodyMultiplicityMessageToNeighbours() {
    LOGIC(m_pSimulationLog,"MPI>\t Send message (SPLITBODYFACTOR_MESSAGE) to neighbours!"<<std::endl;)
    for(auto it = m_nbRanksSendRecvLocal.begin(); it != m_nbRanksSendRecvLocal.end(); ++it) {
        LOGIC(m_pSimulationLog,"--->\t\t Send contact message to neighbours with rank: "<< *it <<std::endl;)
        // Instanciate a MessageWrapper which contains a boost::serialization function!
        m_messageMultiplicity.setRank(*it);
        m_pProcComm->sendMessageToNeighbourRank_async(m_messageMultiplicity,*it, m_messageMultiplicity.m_tag );
    }
    LOGIC(m_pSimulationLog,"MPI>\t Send finished!"<<std::endl;)
}

template<typename TCombo>
void InclusionCommunicator<TCombo>::recvBodyMultiplicityMessageFromNeighbours() {
    LOGIC(m_pSimulationLog,"MPI>\t Receive all messages (SPLITBODYFACTOR_MESSAGE) from neighbours!"<<std::endl;)
    if(m_nbRanksSendRecvRemote.size() == 0){return;}
    // set the rank of the receiving message automatically! inside the function!
    // m_nbRanksSendRecvRem#ote.erase(m_nbRanksSendRecvRemote.begin()); with this in, it obviously hangs!!
    // so the code is correct! all messages get received
    m_pProcComm->receiveMessageFromRanks(m_messageMultiplicity, m_nbRanksSendRecvRemote, m_messageMultiplicity.m_tag );
    LOGIC(m_pSimulationLog,"MPI>\t Receive finished!"<<std::endl;)

    // Wait for all sends to complete, Important because we issue a nonblocking send in sendMessagesToNeighbours
    // We dont send to all neighbours but this should not be a problem for the underlying MPI_Wait call
    m_pProcComm->waitForAllNeighbourSends();
}


template<typename TCombo>
void InclusionCommunicator<TCombo>::sendUpdateSplitBodiesToNeighbours() {
    LOGIC(m_pSimulationLog,"MPI>\t Send message (SPLITBODYUPDATE_MESSAGE) to neighbours!"<<std::endl;)

    m_messageSplitBodyUpdate.setStep(m_globalIterationNumber);
    for(auto it = m_nbRanksSendRecvRemote.begin(); it != m_nbRanksSendRecvRemote.end(); ++it) {
        LOGIC(m_pSimulationLog,"--->\t\t Send contact message to neighbours with rank: "<< *it <<std::endl;)
        // Instanciate a MessageWrapper which contains a boost::serialization function!
        m_messageSplitBodyUpdate.setRank(*it);
        m_pProcComm->sendMessageToNeighbourRank_async(m_messageSplitBodyUpdate,*it, m_messageSplitBodyUpdate.m_tag );
    }
    LOGIC(m_pSimulationLog,"MPI>\t Send finished!"<<std::endl;)
}

template<typename TCombo>
void InclusionCommunicator<TCombo>::recvUpdateSplitBodiesFromNeighbours() {
    LOGIC(m_pSimulationLog,"MPI>\t Receive all messages (SPLITBODYUPDATE_MESSAGE) from neighbours!"<<std::endl;)
    if(m_nbRanksSendRecvLocal.size() == 0){return;}
    // set the rank of the receiving message automatically! inside the function!
    // m_nbRanksSendRecv##Remote.erase(m_nbRanksSendRecvRemote.begin()); with this in, it obviously hangs!!
    // so the code is correct! all messages get received
    m_pProcComm->receiveMessageFromRanks(m_messageSplitBodyUpdate, m_nbRanksSendRecvLocal,
                                        m_messageSplitBodyUpdate.m_tag );
    LOGIC(m_pSimulationLog,"MPI>\t Receive finished!"<<std::endl;)

    // Wait for all sends to complete, Important because we issue a nonblocking send in sendMessagesToNeighbours
    // We dont send to all neighbours but this should not be a problem for the underlying MPI_Wait call
    m_pProcComm->waitForAllNeighbourSends();
}

template<typename TCombo>
void InclusionCommunicator<TCombo>::sendSolutionSplitBodiesToNeighbours() {
    LOGIC(m_pSimulationLog,"MPI>\t Send message (SPLITBODYSOLUTION_MESSAGE) to neighbours!"<<std::endl;)
    m_messageSplitBodySolution.setStep(m_globalIterationNumber);

    for(auto it = m_nbRanksSendRecvLocal.begin(); it != m_nbRanksSendRecvLocal.end(); ++it) {
        LOGIC(m_pSimulationLog,"--->\t\t Send contact message to neighbours with rank: "<< *it <<std::endl;)
        // Instanciate a MessageWrapper which contains a boost::serialization function!
        m_messageSplitBodySolution.setRank(*it);
        m_pProcComm->sendMessageToNeighbourRank_async(m_messageSplitBodySolution,*it, m_messageSplitBodySolution.m_tag );
    }
    LOGIC(m_pSimulationLog,"MPI>\t Send finished!"<<std::endl;)
}

template<typename TCombo>
void InclusionCommunicator<TCombo>::recvSolutionSplitBodiesFromNeighbours() {
    LOGIC(m_pSimulationLog,"MPI>\t Receive all messages (SPLITBODYSOLUTION_MESSAGE) from neighbours!"<<std::endl;)
    if(m_nbRanksSendRecvRemote.size() == 0){return;}
    // set the rank of the receiving message automatically! inside the function!
    // m_nbRanksSendRecvRem#ote.erase(m_nbRanksSendRecvRemote.begin()); with this in, it obviously hangs!!
    // so the code is correct! all messages get received
    m_pProcComm->receiveMessageFromRanks(m_messageSplitBodySolution, m_nbRanksSendRecvRemote,
                                        m_messageSplitBodySolution.m_tag );
    LOGIC(m_pSimulationLog,"MPI>\t Receive finished!"<<std::endl;)

    // Wait for all sends to complete, Important because we issue a nonblocking send in sendMessagesToNeighbours
    // We dont send to all neighbours but this should not be a problem for the underlying MPI_Wait call
    m_pProcComm->waitForAllNeighbourSends();
}
